{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As a data scientist at HBO, I wish to collect information from 2 subreddits on Reddit containing posts from our biggest competitor, Netflix, as a comparison. By using NLP to train a classifier, the use cases for this info may be used for the following:\n",
    "\n",
    "- Identify popular shows and genres\n",
    "    + identify which aspects of the show resonate most strongly with fans by analysing language used in posts/comments\n",
    "- Sentiment analysis\n",
    "    + identify potential areas of improvement in shows with negative sentiment\n",
    "    + assess the appeal of shows to a specific demographic, impact decision-making around content development\n",
    "    + monitor brand reputation\n",
    "- Optimise marketing messages and ad campaigns\n",
    "    + identify key messages that resonate with target audience\n",
    "    + identify potential areas for product differentiation\n",
    "- Track competitor performance\n",
    "    + insights into strengths and weaknesses of competitor\n",
    "- Detect emerging trends\n",
    "    + capitalise on any surge in posts relating to a genre/theme by developing new shows/ promote existing shows\n",
    "- Identify content gaps\n",
    "    + any gaps driving audiences to other competitors?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "1. Data Scraping\n",
    "2. Data Cleaning\n",
    "3. EDA\n",
    "4. Modeling\n",
    "5. Evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from psaw import PushshiftAPI \n",
    "import datetime as dt            \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = int(dt.datetime(2023,3,6,0,0).timestamp())\n",
    "after = int(dt.datetime(2023,2,6,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"hbo\"\n",
    "limit=100\n",
    "submissions = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(submissions)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>hbo</td>\n",
       "      <td>hbo</td>\n",
       "      <td>hbo</td>\n",
       "      <td>hbo</td>\n",
       "      <td>hbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftext</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_fullname</th>\n",
       "      <td>t2_4cucgawq2</td>\n",
       "      <td>t2_4cucgawq2</td>\n",
       "      <td>t2_4cucgawq2</td>\n",
       "      <td>t2_4cucgawq2</td>\n",
       "      <td>t2_4cucgawq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gilded</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>\"F*ck’s sake man, you’re amateur\": Christian B...</td>\n",
       "      <td>\"F*ck’s sake man, you’re amateur\": Christian B...</td>\n",
       "      <td>The secret history of ‘Gone with the Wind’: A ...</td>\n",
       "      <td>Pedro Pascal Thinks He Was More Comfortable On...</td>\n",
       "      <td>Clancy Brown Joins Cast of HBO's The Penguin S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_metadata</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gallery_data</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crosspost_parent</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poll_data</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0  \\\n",
       "subreddit                                                            hbo   \n",
       "selftext                                                                   \n",
       "author_fullname                                             t2_4cucgawq2   \n",
       "gilded                                                                 0   \n",
       "title                  \"F*ck’s sake man, you’re amateur\": Christian B...   \n",
       "...                                                                  ...   \n",
       "media_metadata                                                       NaN   \n",
       "gallery_data                                                         NaN   \n",
       "crosspost_parent_list                                                NaN   \n",
       "crosspost_parent                                                     NaN   \n",
       "poll_data                                                            NaN   \n",
       "\n",
       "                                                                       1  \\\n",
       "subreddit                                                            hbo   \n",
       "selftext                                                                   \n",
       "author_fullname                                             t2_4cucgawq2   \n",
       "gilded                                                                 0   \n",
       "title                  \"F*ck’s sake man, you’re amateur\": Christian B...   \n",
       "...                                                                  ...   \n",
       "media_metadata                                                       NaN   \n",
       "gallery_data                                                         NaN   \n",
       "crosspost_parent_list                                                NaN   \n",
       "crosspost_parent                                                     NaN   \n",
       "poll_data                                                            NaN   \n",
       "\n",
       "                                                                       2  \\\n",
       "subreddit                                                            hbo   \n",
       "selftext                                                                   \n",
       "author_fullname                                             t2_4cucgawq2   \n",
       "gilded                                                                 0   \n",
       "title                  The secret history of ‘Gone with the Wind’: A ...   \n",
       "...                                                                  ...   \n",
       "media_metadata                                                       NaN   \n",
       "gallery_data                                                         NaN   \n",
       "crosspost_parent_list                                                NaN   \n",
       "crosspost_parent                                                     NaN   \n",
       "poll_data                                                            NaN   \n",
       "\n",
       "                                                                       3  \\\n",
       "subreddit                                                            hbo   \n",
       "selftext                                                                   \n",
       "author_fullname                                             t2_4cucgawq2   \n",
       "gilded                                                                 0   \n",
       "title                  Pedro Pascal Thinks He Was More Comfortable On...   \n",
       "...                                                                  ...   \n",
       "media_metadata                                                       NaN   \n",
       "gallery_data                                                         NaN   \n",
       "crosspost_parent_list                                                NaN   \n",
       "crosspost_parent                                                     NaN   \n",
       "poll_data                                                            NaN   \n",
       "\n",
       "                                                                       4  \n",
       "subreddit                                                            hbo  \n",
       "selftext                                                                  \n",
       "author_fullname                                             t2_4cucgawq2  \n",
       "gilded                                                                 0  \n",
       "title                  Clancy Brown Joins Cast of HBO's The Penguin S...  \n",
       "...                                                                  ...  \n",
       "media_metadata                                                       NaN  \n",
       "gallery_data                                                         NaN  \n",
       "crosspost_parent_list                                                NaN  \n",
       "crosspost_parent                                                     NaN  \n",
       "poll_data                                                            NaN  \n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_df = pd.DataFrame(submissions)\n",
    "# preview the comments data\n",
    "submissions_df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l3/fj37td4n0v5dx2z6bx4nz9cc0000gn/T/ipykernel_61479/1820467090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmissions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/hbo_submissions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "submissions_df.to_csv('../data/hbo_submissions.csv', header=True, index=False, columns=list(comments_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be scrapping the following subreddits:\n",
    "- [HBO](https://www.reddit.com/r/hbo/)\n",
    "- [Netflix](https://www.reddit.com/r/netflix/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['HBO', 'Netflix']\n",
    "post_counter = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Pushshift](https://github.com/pushshift/api), there are two main endpoints used to search all publicly available comments and submissions on Reddit:\n",
    "\n",
    "- /reddit/search/comment\n",
    "- /reddit/search/submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I have decided to focus on Submissions as I want to identify in which subreddit a particular submission would fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "url2 = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in subreddits:\n",
    "    params = {'subreddit': i,\n",
    "    'size': '250',\n",
    "    'after': 1678168871,\n",
    "    'selftext:not': '[removed]',\n",
    "    'fields': ['subreddit', 'selftext', 'title', ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['subreddit', 'selftext', 'title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function which takes in url.\n",
    "timestamp of 1st and last past, supposed to be more or less recent, so that for next iteration of scrape, also set a param for that timestamp. make sure post we scrape do not overlap\n",
    "at least 3, naive bayes, logistic regression, last (random forest/gradient boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
