{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project - Wine Recommender System <br> [Part 2 of 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4oQlBkKIzq5F"
   },
   "source": [
    "## Contents:\n",
    "- [Overview](##Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDgLbO48JwJX"
   },
   "source": [
    "---\n",
    "## Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iaRZZCcTKq5s"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/wine_reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>vintage</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Medium-gold in color. Complex and inviting nos...</td>\n",
       "      <td>92</td>\n",
       "      <td>80</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Dopff &amp; Irion 2004 Schoenenbourg Grand Cru Ven...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Dopff &amp; Irion</td>\n",
       "      <td>2004</td>\n",
       "      <td>['medium-gold', 'color', 'complex', 'inviting'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Slightly backward, particularly given the vint...</td>\n",
       "      <td>92</td>\n",
       "      <td>70</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Ceretto 2003 Bricco Rocche Prapó  (Barolo)</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>Ceretto</td>\n",
       "      <td>2003</td>\n",
       "      <td>['slightly', 'backward', 'particularly', 'give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>The vineyard is one of the better Chardonnay s...</td>\n",
       "      <td>92</td>\n",
       "      <td>36</td>\n",
       "      <td>California</td>\n",
       "      <td>Alexander Valley</td>\n",
       "      <td>Virginie Boone</td>\n",
       "      <td>Matrix 2007 Stuhlmuller Vineyard Chardonnay (A...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Matrix</td>\n",
       "      <td>2007</td>\n",
       "      <td>['vineyard', 'one', 'better', 'chardonnay', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>Defines Rockpile Zinfandel in intensity of fru...</td>\n",
       "      <td>92</td>\n",
       "      <td>39</td>\n",
       "      <td>California</td>\n",
       "      <td>Rockpile</td>\n",
       "      <td>Virginie Boone</td>\n",
       "      <td>Mauritson 2007 Rockpile Cemetary Vineyard Zinf...</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>Mauritson</td>\n",
       "      <td>2007</td>\n",
       "      <td>['defines', 'rockpile', 'zinfandel', 'intensit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>This sophisticated wine is mostly Cabernet Sau...</td>\n",
       "      <td>92</td>\n",
       "      <td>45</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Virginie Boone</td>\n",
       "      <td>Silverado 2006 Cabernet Sauvignon (Napa Valley)</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Silverado</td>\n",
       "      <td>2006</td>\n",
       "      <td>['sophisticated', 'mostly', 'cabernet', 'sauvi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  points  price  \\\n",
       "0  France  Medium-gold in color. Complex and inviting nos...      92     80   \n",
       "1   Italy  Slightly backward, particularly given the vint...      92     70   \n",
       "2      US  The vineyard is one of the better Chardonnay s...      92     36   \n",
       "3      US  Defines Rockpile Zinfandel in intensity of fru...      92     39   \n",
       "4      US  This sophisticated wine is mostly Cabernet Sau...      92     45   \n",
       "\n",
       "     province          region_1       taster_name  \\\n",
       "0      Alsace            Alsace  Anne Krebiehl MW   \n",
       "1    Piedmont            Barolo     Kerin O’Keefe   \n",
       "2  California  Alexander Valley    Virginie Boone   \n",
       "3  California          Rockpile    Virginie Boone   \n",
       "4  California       Napa Valley    Virginie Boone   \n",
       "\n",
       "                                               title             variety  \\\n",
       "0  Dopff & Irion 2004 Schoenenbourg Grand Cru Ven...            Riesling   \n",
       "1         Ceretto 2003 Bricco Rocche Prapó  (Barolo)            Nebbiolo   \n",
       "2  Matrix 2007 Stuhlmuller Vineyard Chardonnay (A...          Chardonnay   \n",
       "3  Mauritson 2007 Rockpile Cemetary Vineyard Zinf...           Zinfandel   \n",
       "4    Silverado 2006 Cabernet Sauvignon (Napa Valley)  Cabernet Sauvignon   \n",
       "\n",
       "          winery vintage                                             tokens  \n",
       "0  Dopff & Irion    2004  ['medium-gold', 'color', 'complex', 'inviting'...  \n",
       "1        Ceretto    2003  ['slightly', 'backward', 'particularly', 'give...  \n",
       "2         Matrix    2007  ['vineyard', 'one', 'better', 'chardonnay', 's...  \n",
       "3      Mauritson    2007  ['defines', 'rockpile', 'zinfandel', 'intensit...  \n",
       "4      Silverado    2006  ['sophisticated', 'mostly', 'cabernet', 'sauvi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the description into words\n",
    "\n",
    "corpus = []\n",
    "for words in df['tokens']:\n",
    "    corpus.append(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to generate average word embeddings for each wine description\n",
    "\n",
    "def average_word_embeddings(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate average word embeddings for all wine descriptions\n",
    "\n",
    "vocabulary = set(word2vec_model.index_to_key)\n",
    "num_features = 300\n",
    "\n",
    "wine_embeddings = list(map(lambda x: average_word_embeddings(x, word2vec_model, vocabulary, num_features), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# calculate cosine similarity between wine descriptions using embeddings\n",
    "\n",
    "wine_embeddings_matrix = np.vstack(wine_embeddings)\n",
    "cosine_sim_word2vec = cosine_similarity(wine_embeddings_matrix, wine_embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title, cosine_sim_matrix):\n",
    "\n",
    "    # Taking the title and creating a new dataframe called wines\n",
    "    wines = df[['title']]\n",
    "    \n",
    "    # Reverse mapping of the index\n",
    "    indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "         \n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    wine_indices = [i[0] for i in sim_scores]\n",
    "    recommend = wines.iloc[wine_indices]\n",
    "    \n",
    "    print(\"Top 10 wine recommendations for '{}':\".format(title))\n",
    "    for index, row in recommend.iterrows():\n",
    "        print(\"- {}\".format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 wine recommendations for 'Dopff & Irion 2004 Schoenenbourg Grand Cru Vendanges Tardives Riesling (Alsace)':\n",
      "- Ceretto 2003 Bricco Rocche Prapó  (Barolo)\n",
      "- Matrix 2007 Stuhlmuller Vineyard Chardonnay (Alexander Valley)\n",
      "- Mauritson 2007 Rockpile Cemetary Vineyard Zinfandel (Rockpile)\n",
      "- Silverado 2006 Cabernet Sauvignon (Napa Valley)\n",
      "- Le Riche 2003 Cabernet Sauvignon Reserve Cabernet Sauvignon (Stellenbosch)\n",
      "- Pierre Sparr 2007 Vendages Tardives Gewurztraminer (Alsace)\n",
      "- Pierre Sparr 2008 Alsace One White (Alsace)\n",
      "- Kuentz-Bas 2008 Pinot Blanc (Alsace)\n",
      "- Camberley 2004 Philosophers' Stone Red (Stellenbosch)\n",
      "- Ceretto 2003 Bricco Rocche Brunate  (Barolo)\n"
     ]
    }
   ],
   "source": [
    "# Test recommendation system\n",
    "sample_wine = df.iloc[0]['title']\n",
    "recommendations(sample_wine, cosine_sim_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: word_tokenize(x\u001b[39m.\u001b[39;49mlower()))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: word_tokenize(x\u001b[39m.\u001b[39;49mlower()))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1273\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1321\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1322\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[1;32m   1323\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1421\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1422\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[1;32m   1423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    316\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m   1394\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1395\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[1;32m   1396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1397\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1373\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1374\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1375\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[1;32m   1376\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[1;32m   1378\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model\n",
    "w2v = Word2Vec(df['tokens'], min_count=1)\n",
    "\n",
    "# Compute Average Word2Vec embeddings\n",
    "def average_w2v(tokens, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv.key_to_index:  # Updated to use key_to_index instead of vocab\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "    if n_words:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "avg_w2v_matrix = np.zeros((len(df), w2v.vector_size))\n",
    "for i, tokens in enumerate(df['tokens']):\n",
    "    avg_w2v_matrix[i] = average_w2v(tokens, w2v, w2v.vector_size)\n",
    "\n",
    "# Compute TF-IDF Word2Vec embeddings\n",
    "dct = Dictionary(df['tokens'])\n",
    "corpus = [dct.doc2bow(line) for line in df['tokens']]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_w2v_matrix = np.zeros((len(df), w2v.vector_size))\n",
    "\n",
    "for i, tokens in enumerate(df['tokens']):\n",
    "    weighted_vec = np.zeros((w2v.vector_size,), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in tokens:\n",
    "        if word in w2v.wv.key_to_index:  # Updated to use key_to_index instead of vocab\n",
    "            word_vec = w2v.wv[word]\n",
    "            word_tfidf_list = tfidf_model[dct.doc2bow([word])]\n",
    "            if word_tfidf_list:\n",
    "                word_tfidf = word_tfidf_list[0][1]\n",
    "                weighted_vec += word_vec * word_tfidf\n",
    "                n_words += 1\n",
    "    if n_words:\n",
    "        weighted_vec /= n_words\n",
    "    tfidf_w2v_matrix[i] = weighted_vec\n",
    "\n",
    "# Compute cosine similarity for both matrices\n",
    "cs_avg = cosine_similarity(avg_w2v_matrix, avg_w2v_matrix)\n",
    "cs_tfidf = cosine_similarity(tfidf_w2v_matrix, tfidf_w2v_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation functions\n",
    "def recommend_wines_avg(user_input, n=10):\n",
    "    user_tokens = preprocess_text(user_input)\n",
    "    user_vec = average_word2vec(user_tokens, w2v, w2v.vector_size).reshape(1, -1)\n",
    "    sim_scores = cs_avg.dot(user_vec.T).flatten()\n",
    "    top_indices = sim_scores.argsort()[-n:][::-1]\n",
    "    return df.iloc[top_indices]\n",
    "\n",
    "def recommend_wines_tfidf(user_input, n=10):\n",
    "    user_tokens = preprocess_text(user_input)\n",
    "    user_vec = average_word2vec(user_tokens, w2v, w2v.vector_size).reshape(1, -1)\n",
    "    sim_scores = cs_tfidf.dot(user_vec.T).flatten()\n",
    "    top_indices = sim_scores.argsort()[-n:][::-1]\n",
    "    return df.iloc[top_indices]\n",
    "\n",
    "# Test the recommendation engines\n",
    "user_input = \"cherry grape fruity\"\n",
    "recommended_wines_avg = recommend_wines_avg(user_input, n=10)\n",
    "recommended_wines_tfidf = recommend_wines_tfidf(user_input, n=10)\n",
    "\n",
    "print(\"Recommendations based on Average Word2Vec:\")\n",
    "print(recommended_wines_avg[['title', 'variety', 'points', 'price']])\n",
    "\n",
    "print(\"\\nRecommendations based on TF-IDF Word2Vec:\")\n",
    "print(recommended_wines_tfidf[['title', 'variety', 'points', 'price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "72a51f30bf8c483e1fc0abe989f81cc66824b206d24365642f1a7e7f70f5a428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
